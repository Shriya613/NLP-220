# -*- coding: utf-8 -*-
"""220_PartC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PjVXp3FFLuigUKV2b4qpDBX6wMvedzyH
"""

import numpy as np
import pandas as pd
from collections import defaultdict
import math
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB

df = pd.read_csv('/content/small_books_rating.csv')

df.columns

df = df[(df['review/score'] <= 2) | (df['review/score'] >= 4)]
df['label'] = df['review/score'].apply(lambda x: 1 if x >= 4 else 0)

# Extract the relevant columns: 'review/text' and 'label'
reviews = df['review/text'].fillna('').values
labels = df['label'].values

# Split into train and test set
X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.15, random_state=42, shuffle=True)

class CustomNaiveBayes:
    def __init__(self):
        self.class_priors = {}
        self.feature_likelihoods = defaultdict(lambda: defaultdict(float))
        self.vocab = set()
        self.class_counts = defaultdict(int)
        self.total_docs = 0

    def fit(self, X, y):
        self.total_docs = len(y)
        # Calculate priors and feature likelihoods
        for features, label in zip(X, y):
            self.class_counts[label] += 1
            for feature in features:
                self.vocab.add(feature)
                self.feature_likelihoods[label][feature] += 1

        # Calculate class priors P(Y)
        for label in self.class_counts:
            self.class_priors[label] = self.class_counts[label] / self.total_docs

        # Calculate feature likelihoods P(X|Y)
        for label, feature_counts in self.feature_likelihoods.items():
            total_count = sum(feature_counts.values())
            for feature in self.vocab:
                # Add 1 for Laplace smoothing
                self.feature_likelihoods[label][feature] = (feature_counts[feature] + 1) / (total_count + len(self.vocab))

    def predict(self, X):
        predictions = []
        for features in X:
            class_probs = {}
            for label in self.class_priors:
                # Start with the log of the prior probability
                log_prob = math.log(self.class_priors[label])

                # Add the log likelihoods for each feature
                for feature in features:
                    if feature in self.vocab:
                        log_prob += math.log(self.feature_likelihoods[label].get(feature, 1 / (len(self.vocab))))

                class_probs[label] = log_prob

            # Choose the class with the highest probability (0 or 1)
            predictions.append(max(class_probs, key=class_probs.get))
        return predictions

def evaluate_model(X_train, X_test, y_train, y_test, vectorizer, model_name):
    # Fit the vectorizer on the training data and transform both train and test data
    X_train_vec = vectorizer.fit_transform(X_train).toarray()
    X_test_vec = vectorizer.transform(X_test).toarray()

    # Convert the vectorized data into list of features (used by Custom Naive Bayes)
    feature_names = vectorizer.get_feature_names_out()
    X_train_features = [[feature_names[i] for i, val in enumerate(doc) if val > 0] for doc in X_train_vec]
    X_test_features = [[feature_names[i] for i, val in enumerate(doc) if val > 0] for doc in X_test_vec]

    # Custom Naive Bayes
    custom_nb = CustomNaiveBayes()
    custom_nb.fit(X_train_features, y_train)
    y_pred_custom = custom_nb.predict(X_test_features)

    custom_accuracy = accuracy_score(y_test, y_pred_custom)
    custom_f1 = f1_score(y_test, y_pred_custom, average='macro')

    print(f"Custom Naive Bayes {model_name} Accuracy:", custom_accuracy)
    print(f"Custom Naive Bayes {model_name} Macro F1 Score:", custom_f1)
    print(f"Custom Naive Bayes {model_name} Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred_custom))

    # Scikit-learn Naive Bayes
    clf_nb = MultinomialNB()
    clf_nb.fit(X_train_vec, y_train)
    y_pred_sklearn = clf_nb.predict(X_test_vec)

    sklearn_accuracy = accuracy_score(y_test, y_pred_sklearn)
    sklearn_f1 = f1_score(y_test, y_pred_sklearn, average='macro')

    print(f"\nScikit-learn Naive Bayes {model_name} Accuracy:", sklearn_accuracy)
    print(f"Scikit-learn Naive Bayes {model_name} Macro F1 Score:", sklearn_f1)
    print(f"Scikit-learn Naive Bayes {model_name} Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred_sklearn))

# Bag of Words
bow_vectorizer = CountVectorizer(stop_words='english', max_features=5000)
evaluate_model(X_train, X_test, y_train, y_test, bow_vectorizer, "BOW")

# n-grams (bigrams)
ngram_vectorizer = CountVectorizer(stop_words='english', ngram_range=(2, 2), max_features=5000)
evaluate_model(X_train, X_test, y_train, y_test, ngram_vectorizer, "n-grams")

# TF-IDF
tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
evaluate_model(X_train, X_test, y_train, y_test, tfidf_vectorizer, "TF-IDF")

import matplotlib.pyplot as plt
import numpy as np

# Sample data (replace these with your actual results)
feature_types = ['BOW', 'n-grams', 'TF-IDF']
custom_accuracies = [0.8422, 0.7918, 0.8422]  # Replace with actual custom Naive Bayes accuracies
sklearn_accuracies = [0.8338, 0.8340, 0.8518]  # Replace with actual sklearn Naive Bayes accuracies

# Set up the plot
x = np.arange(len(feature_types))  # the label locations
width = 0.35  # the width of the bars

fig, ax = plt.subplots(figsize=(10, 6))
bars1 = ax.bar(x - width/2, custom_accuracies, width, label='Custom Naive Bayes', color='skyblue')
bars2 = ax.bar(x + width/2, sklearn_accuracies, width, label='Scikit-Learn Naive Bayes', color='salmon')

# Add labels, title, and custom x-axis tick labels
ax.set_xlabel('Feature Extraction Method')
ax.set_ylabel('Accuracy')
ax.set_title('Comparison of Accuracy: Custom vs Scikit-Learn Naive Bayes')
ax.set_xticks(x)
ax.set_xticklabels(feature_types)
ax.legend()

# Adding value labels on top of bars
def add_labels(bars):
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.4f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

add_labels(bars1)
add_labels(bars2)

plt.show()

