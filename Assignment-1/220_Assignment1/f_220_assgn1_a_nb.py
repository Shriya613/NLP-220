# -*- coding: utf-8 -*-
"""F_220_assgn1_A_NB

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_SoTqsUURCVFh2L63XRXwwJ2PP9hKaDg

N-grams (bi-grams)
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
import sklearn.linear_model
import sklearn.metrics

#Binary classification
df = pd.read_csv('/content/small_books_rating.csv')

df.head()

df.info()

df.describe()

print(df['review/score'].value_counts())

# 1. Distribution of review scores
plt.figure(figsize=(8, 5))
sns.countplot(x='review/score', data=df, hue='review/score', legend=False, palette='icefire')
plt.title('Distribution of Review Scores')
plt.xlabel('Review Score')
plt.ylabel('Count')
plt.show()

# 2. Length distribution of review text
df['review_text_length'] = df['review/text'].apply(lambda x: len(str(x)))
plt.figure(figsize=(10, 6))
sns.histplot(df['review_text_length'], bins=30, kde=True, color='purple')
plt.title('Distribution of Review Text Length')
plt.xlabel('Text Length (characters)')
plt.ylabel('Frequency')
plt.show()

# 3. Word count in review summaries
df['summary_word_count'] = df['review/summary'].apply(lambda x: len(str(x).split()))
plt.figure(figsize=(10, 6))
sns.histplot(df['summary_word_count'], bins=30, kde=True, color='teal')
plt.title('Distribution of Word Count in Review Summaries')
plt.xlabel('Word Count')
plt.ylabel('Frequency')
plt.show()

# 4. Count of reviews per book title
plt.figure(figsize=(8,5))
top_titles = df['Title'].value_counts().nlargest(5)  # Top 5 books by review count
# sns.countplot(x='review/score', data=df, hue='review/score', legend=False, palette='viridis')  # Removed unnecessary countplot
sns.barplot(x=top_titles.index, y=top_titles.values, legend= False, palette='coolwarm')
# Added data parameter and filtered to include only the top 5 titles
plt.title('Top 5 Most Reviewed Book Titles')
plt.xlabel('Number of Reviews')
plt.ylabel('Book Title')
plt.xticks(rotation=15, ha='right')
plt.show()

# 5. Correlation heatmap (for numerical columns)
plt.figure(figsize=(8, 6))
# Select only numerical features for correlation calculation
numerical_df = df.select_dtypes(include=['number'])
sns.heatmap(numerical_df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix of Numerical Features')
plt.show()

list(df.columns)

print("\nMissing Values in Each Column:")
print(df.isnull().sum())

# Fill missing values in text columns with an empty string
df['review/text'] = df['review/text'].fillna("")
df['review/summary'] = df['review/summary'].fillna("")
df['Title'] = df['Title'].fillna("Unknown Title")

# For any numerical columns with missing values, you could use the mean or median
# Example: df['some_numeric_column'] = df['some_numeric_column'].fillna(df['some_numeric_column'].mean())

# Verify there are no remaining missing values
print(df.isnull().sum())

# Define the function to plot class distribution
import numpy as np

def plot_class_distribution(y_train, y_test, labels=None):
    # Count occurrences of each class in y_train and y_test
    unique, train_counts = np.unique(y_train, return_counts=True)
    _, test_counts = np.unique(y_test, return_counts=True)

    # Bar plot for training and testing distributions
    x = np.arange(len(unique))  # label locations
    width = 0.35  # bar width

    plt.figure(figsize=(10, 6))
    plt.bar(x - width/2, train_counts, width, label='Train', color='skyblue')
    plt.bar(x + width/2, test_counts, width, label='Test', color='salmon')

    # Add labels, title, and legend
    plt.xlabel("Class Label")
    plt.ylabel("Count")
    plt.title("Class/Label Distribution in Train and Test Sets")
    plt.xticks(x, labels if labels else unique)
    plt.legend()

    plt.show()

# Add the class distribution plot here
plot_class_distribution(y_train, y_test)

df = df[(df['review/score'] <= 2) | (df['review/score'] >= 4)]

df['label'] = df['review/score'].apply(lambda x: 1 if x >= 4 else 0)

X = df['review/text']
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, shuffle=True)

#Feature Engineering - N-grams (bi-grams)
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=5000)
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
from time import time

def evaluate_model(model, X_train, X_test, y_train, y_test):
    start_train = time()
    model.fit(X_train, y_train)
    end_train = time()
    training_time = end_train - start_train

    start_pred = time()
    y_pred = model.predict(X_test)
    end_pred = time()
    inference_time = end_pred - start_pred

    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='macro')
    f1_each_class = f1_score(y_test, y_pred, average = None)
    conf_matrix = confusion_matrix(y_test, y_pred)

    print(f"Model: {model.__class__.__name__}")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 Score (for each class): {f1_each_class}")
    print(f"Macro F1 Score: {f1:.4f}")
    print(f"Training Time: {training_time:.4f} seconds")
    print(f"Inference Time: {inference_time:.4f} seconds")
    print(f"Confusion Matrix:\n{conf_matrix}")
    print("-" * 40)

    plt.figure(figsize=(6,4))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title(f"Confusion Matrix for {model.__class__.__name__}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

    return accuracy, f1, training_time, inference_time

# Initialize models
from sklearn.naive_bayes import MultinomialNB
nb_model = MultinomialNB()

# Evaluate models
nb_results = evaluate_model(nb_model, X_train_vec, X_test_vec, y_train, y_test)

sns.countplot(x=y)
plt.title('Class Distribution')
plt.xlabel('Class')
plt.ylabel('Frequency')
plt.show()

results_df = pd.DataFrame({
    'Model': ['Naive Bayes (N-Grams)'],
    'Accuracy': [nb_results[0]],
    'Macro F1 Score': [nb_results[1]],
    'Training Time (s)': [nb_results[2]],
    'Inference Time (s)': [nb_results[3]]
})

print(results_df)

"""Bag-of-Words"""

df = df[(df['review/score'] <= 2) | (df['review/score'] >= 4)]
df['label'] = df['review/score'].apply(lambda x: 1 if x >= 4 else 0)
X = df['review/text']
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, shuffle=True)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
def evaluate_model(model, X_train, X_test, y_train, y_test):
    start_train = time()
    model.fit(X_train, y_train)
    end_train = time()
    training_time = end_train - start_train

    start_pred = time()
    y_pred = model.predict(X_test)
    end_pred = time()
    inference_time = end_pred - start_pred

    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='macro')
    f1_each_class = f1_score(y_test, y_pred, average = None)
    conf_matrix = confusion_matrix(y_test, y_pred)

    print(f"Model: {model.__class__.__name__}")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 Score (for each class): {f1_each_class}")
    print(f"Macro F1 Score: {f1:.4f}")
    print(f"Training Time: {training_time:.4f} seconds")
    print(f"Inference Time: {inference_time:.4f} seconds")
    print(f"Confusion Matrix:\n{conf_matrix}")
    print("-" * 40)

    plt.figure(figsize=(6,4))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title(f"Confusion Matrix for {model.__class__.__name__}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

    return accuracy, f1, training_time, inference_time

# Bag of Words (BOW) feature extraction
from sklearn.feature_extraction.text import CountVectorizer
bow_vectorizer = CountVectorizer(max_features=5000)
X_train_bow = bow_vectorizer.fit_transform(X_train)
X_test_bow = bow_vectorizer.transform(X_test)

#Initialize models
from sklearn.naive_bayes import MultinomialNB
nb_model = MultinomialNB()

print("Evaluating Naive Bayes with BOW Features")
bow_results = evaluate_model(nb_model, X_train_bow, X_test_bow, y_train, y_test)

# Plot class distribution
sns.countplot(x=y)
plt.title('Class Distribution')
plt.xlabel('Class')
plt.ylabel('Frequency')
plt.show()

# Store results in DataFrame
results_df = pd.DataFrame({
    'Model': ['Naive Bayes (BOW)'],
    'Accuracy': [bow_results[0]],
    'Macro F1 Score': [bow_results[1]],
    'Training Time (s)': [bow_results[2]],
    'Inference Time (s)': [bow_results[3]]
})

print(results_df)

"""Tf-Idf"""

# Step 3: Feature Engineering - TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Step 5: Model Training and Evaluation
def evaluate_model(model, X_train, X_test, y_train, y_test):
    start_train = time()
    model.fit(X_train, y_train)
    end_train = time()
    training_time = end_train - start_train

    start_pred = time()
    y_pred = model.predict(X_test)
    end_pred = time()
    inference_time = end_pred - start_pred

    accuracy = accuracy_score(y_test, y_pred)
    f1_each_class = f1_score(y_test, y_pred, average = None)
    f1 = f1_score(y_test, y_pred, average='macro')
    conf_matrix = confusion_matrix(y_test, y_pred)

    print(f"Model: {model.__class__.__name__}")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 Score (for each class): {f1_each_class}")
    print(f"Macro F1 Score: {f1:.4f}")
    print(f"Training Time: {training_time:.4f} seconds")
    print(f"Inference Time: {inference_time:.4f} seconds")
    print(f"Confusion Matrix:\n{conf_matrix}")
    print("-" * 40)

    plt.figure(figsize=(6,4))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title(f"Confusion Matrix for {model.__class__.__name__}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

    return accuracy, f1, training_time, inference_time

nb_model = MultinomialNB()

print("Evaluating Naive Bayes with TF-IDF Features")
tfidf_results = evaluate_model(nb_model, X_train_tfidf, X_test_tfidf, y_train, y_test)

sns.countplot(x=y)
plt.title('Class Distribution')
plt.xlabel('Class')
plt.ylabel('Frequency')
plt.show()

results_df = pd.DataFrame({
    'Model': [ 'Naive Bayes (TF-IDF)'],
    'Accuracy': [tfidf_results[0]],
    'Macro F1 Score': [tfidf_results[1]],
    'Training Time (s)': [tfidf_results[2]],
    'Inference Time (s)': [tfidf_results[3]]
})

print(results_df)

